# deepseek

import requests
import os
import time
import re
import hashlib
import threading
from queue import Queue
from typing import List, Optional
from dataclasses import dataclass
from datetime import datetime


@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœ"""

    url: str
    success: bool
    filename: Optional[str] = None
    filepath: Optional[str] = None
    content_length: int = 0
    error_msg: Optional[str] = None


def sanitize_filename(url: str) -> str:
    """
    æ¸…ç†URLç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å

    Args:
        url: ç½‘å€

    Returns:
        å®‰å…¨çš„æ–‡ä»¶å
    """
    # ç§»é™¤åè®®å’Œç‰¹æ®Šå­—ç¬¦
    safe_name = re.sub(r'https?://', '', url)
    safe_name = re.sub(r'[<>:"/\\|?*]', '_', safe_name)
    # safe_name = safe_name[:30]  # é™åˆ¶é•¿åº¦

    # å¦‚æœåŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œä½¿ç”¨MD5å“ˆå¸Œ
    if any(ord(c) > 127 for c in safe_name) or len(safe_name) < 5:
        url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()[:8]
        safe_name = f"page_{url_hash}"

    return f"{safe_name}.html"


class WebCrawlerThread(threading.Thread):
    """çˆ¬è™«å·¥ä½œçº¿ç¨‹"""

    def __init__(
        self,
        thread_id: int,
        url_queue: Queue,
        result_queue: Queue,
        save_dir: str,
        timeout: int = 10,
        max_retries: int = 2,
    ) -> None:
        """
        åˆå§‹åŒ–å·¥ä½œçº¿ç¨‹

        Args:
            thread_id: çº¿ç¨‹ID
            url_queue: URLé˜Ÿåˆ—
            result_queue: ç»“æœé˜Ÿåˆ—
            save_dir: ä¿å­˜ç›®å½•
            timeout: è¶…æ—¶æ—¶é—´
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        super().__init__()
        self.thread_id = thread_id
        self.url_queue = url_queue
        self.result_queue = result_queue
        self.save_dir = save_dir
        self.timeout = timeout
        self.max_retries = max_retries

        # åˆ›å»ºä¼šè¯
        self.session = requests.Session()
        self.session.headers.update(
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            }
        )

    def run(self) -> None:
        """çº¿ç¨‹è¿è¡Œå‡½æ•°"""
        while not self.url_queue.empty():
            try:
                url = self.url_queue.get_nowait()
            except:
                break  # é˜Ÿåˆ—ä¸ºç©ºï¼Œé€€å‡ºçº¿ç¨‹

            result = self.crawl_single(url)
            self.result_queue.put(result)
            self.url_queue.task_done()

        # çº¿ç¨‹ç»“æŸï¼Œå…³é—­ä¼šè¯
        self.session.close()

    def crawl_single(self, url: str) -> CrawlResult:
        """
        çˆ¬å–å•ä¸ªURL

        Args:
            url: ç›®æ ‡URL

        Returns:
            çˆ¬å–ç»“æœ
        """
        for retry in range(self.max_retries + 1):
            try:
                # å‘é€è¯·æ±‚
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()

                # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
                response.encoding = response.apparent_encoding

                # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
                filename = sanitize_filename(url)
                filepath = os.path.join(self.save_dir, filename)

                # ä¿å­˜HTMLå†…å®¹
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(response.text)

                return CrawlResult(
                    url=url,
                    success=True,
                    filename=filename,
                    filepath=filepath,
                    content_length=len(response.text),
                )

            except requests.exceptions.RequestException as e:
                if retry < self.max_retries:
                    time.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’
                    continue

                return CrawlResult(url=url, success=False, error_msg=f"è¯·æ±‚å¤±è´¥: {e}")
            except Exception as e:
                return CrawlResult(url=url, success=False, error_msg=f"ä¿å­˜å¤±è´¥: {e}")

        return CrawlResult(url=url, success=False, error_msg="è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")


class MultiThreadCrawler:
    """å¤šçº¿ç¨‹çˆ¬è™«"""

    def __init__(
        self,
        save_dir: str = "html_pages",
        max_threads: int = 5,
        timeout: int = 10,
        max_retries: int = 2,
    ) -> None:
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            save_dir: ä¿å­˜ç›®å½•
            max_threads: æœ€å¤§çº¿ç¨‹æ•°
            timeout: è¶…æ—¶æ—¶é—´
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.save_dir = save_dir
        self.max_threads = max_threads
        self.timeout = timeout
        self.max_retries = max_retries

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)

    def crawl_urls(self, urls: List[str]) -> List[CrawlResult]:
        """
        å¤šçº¿ç¨‹çˆ¬å–URLåˆ—è¡¨

        Args:
            urls: URLåˆ—è¡¨

        Returns:
            çˆ¬å–ç»“æœåˆ—è¡¨
        """
        print(f"ğŸš€ å¼€å§‹å¤šçº¿ç¨‹çˆ¬å– {len(urls)} ä¸ªç½‘é¡µ...")
        print(f"ğŸ“Š çº¿ç¨‹æ•°: {self.max_threads}")
        print(f"ğŸ“ ä¿å­˜ç›®å½•: {os.path.abspath(self.save_dir)}")
        print("-" * 60)

        # åˆ›å»ºé˜Ÿåˆ—
        url_queue = Queue()
        result_queue = Queue()

        # å°†URLåŠ å…¥é˜Ÿåˆ—
        for url in urls:
            url_queue.put(url)

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # åˆ›å»ºå¹¶å¯åŠ¨å·¥ä½œçº¿ç¨‹
        threads: List[WebCrawlerThread] = []

        for i in range(min(self.max_threads, len(urls))):
            thread = WebCrawlerThread(
                thread_id=i + 1,
                url_queue=url_queue,
                result_queue=result_queue,
                save_dir=self.save_dir,
                timeout=self.timeout,
                max_retries=self.max_retries,
            )
            thread.start()
            threads.append(thread)
            print(f"ğŸ“¡ å¯åŠ¨çº¿ç¨‹ {i + 1}")

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()

        # æ”¶é›†ç»“æœ
        results: List[CrawlResult] = []
        while not result_queue.empty():
            results.append(result_queue.get())

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        elapsed_time = time.time() - start_time
        success_count = sum(1 for r in results if r.success)

        # æ‰“å°ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ‰ çˆ¬å–å®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“ˆ æ€»ç½‘é¡µæ•°: {len(urls)}")
        print(f"âœ… æˆåŠŸ: {success_count}")
        print(f"âŒ å¤±è´¥: {len(urls) - success_count}")
        print(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f}ç§’")
        print(
            f"âš¡ å¹³å‡é€Ÿåº¦: {len(urls)/elapsed_time:.2f}ä¸ª/ç§’"
            if elapsed_time > 0
            else "è®¡ç®—ä¸­..."
        )

        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        self.print_results_summary(results)

        return results

    def print_results_summary(self, results: List[CrawlResult]) -> None:
        """æ‰“å°ç»“æœæ‘˜è¦"""
        print("\nğŸ“‹ ç»“æœæ‘˜è¦:")
        print("-" * 40)

        for i, result in enumerate(results, 1):
            status = "âœ…" if result.success else "âŒ"
            size_info = (
                f"({result.content_length}å­—ç¬¦)" if result.content_length > 0 else ""
            )
            print(f"{i:3d}. {status} {result.url[:50]:50} {size_info}")

            if not result.success and result.error_msg:
                print(f"    é”™è¯¯: {result.error_msg}")

    def save_report(self, results: List[CrawlResult]) -> None:
        """ä¿å­˜çˆ¬å–æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = os.path.join(self.save_dir, f"crawl_report_{timestamp}.txt")

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("ç½‘é¡µçˆ¬å–æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"çº¿ç¨‹æ•°: {self.max_threads}\n")
            f.write(f"ä¿å­˜ç›®å½•: {os.path.abspath(self.save_dir)}\n\n")

            f.write("è¯¦ç»†ç»“æœ:\n")
            f.write("-" * 60 + "\n")

            for i, result in enumerate(results, 1):
                status = "æˆåŠŸ" if result.success else "å¤±è´¥"
                f.write(f"\n{i}. URL: {result.url}\n")
                f.write(f"   çŠ¶æ€: {status}\n")

                if result.success:
                    f.write(f"   æ–‡ä»¶å: {result.filename}\n")
                    f.write(f"   æ–‡ä»¶å¤§å°: {result.content_length} å­—ç¬¦\n")
                else:
                    f.write(f"   é”™è¯¯: {result.error_msg}\n")

        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


def load_urls_from_file(filename: str) -> List[str]:
    """
    ä»æ–‡ä»¶åŠ è½½URLåˆ—è¡¨

    Args:
        filename: æ–‡ä»¶å

    Returns:
        URLåˆ—è¡¨
    """
    urls: List[str] = []

    if not os.path.exists(filename):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        return urls

    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url and not url.startswith('#'):  # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                    # ç¡®ä¿URLæœ‰åè®®å‰ç¼€
                    if not url.startswith(('http://', 'https://')):
                        url = 'https://' + url
                    urls.append(url)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

    return urls


def main() -> None:
    """ä¸»å‡½æ•°"""
    import sys

    print("=" * 60)
    print("ğŸŒ å¤šçº¿ç¨‹ç½‘é¡µçˆ¬å–å·¥å…·")
    print("=" * 60)

    # é…ç½®å‚æ•°
    max_threads = 5
    save_dir = "html_pages"

    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–é…ç½®
    if len(sys.argv) > 1:
        max_threads = int(sys.argv[1])
    if len(sys.argv) > 2:
        save_dir = sys.argv[2]

    # è·å–URLåˆ—è¡¨
    urls: List[str] = []

    # æ–¹æ³•1: ä»æ–‡ä»¶è¯»å–
    if os.path.exists("urls.txt"):
        print("ğŸ“ ä» urls.txt æ–‡ä»¶åŠ è½½URL...")
        urls = load_urls_from_file("urls.txt")

    # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œä½¿ç”¨ç¤ºä¾‹URL
    if not urls:
        print("â„¹ï¸  ä½¿ç”¨ç¤ºä¾‹URLåˆ—è¡¨")
        urls = ['https://www.baidu.com', 'https://www.bilibili.com']

    if not urls:
        print("âŒ æ²¡æœ‰è¦çˆ¬å–çš„URL")
        return

    print(f"ğŸ“„ å‡†å¤‡çˆ¬å– {len(urls)} ä¸ªç½‘é¡µ")

    # åˆ›å»ºçˆ¬è™«
    crawler = MultiThreadCrawler(
        save_dir=save_dir, max_threads=max_threads, timeout=10, max_retries=1
    )

    # å¼€å§‹çˆ¬å–
    results = crawler.crawl_urls(urls)

    # ä¿å­˜æŠ¥å‘Š
    crawler.save_report(results)


if __name__ == "__main__":
    main()
